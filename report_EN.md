# Project Report: Personalized Text Autocompletion Design and Implementation

## 1. Project Overview

Currently, many text editors and programming tools (such as Cursor, Copilot, etc.) have begun to use autocompletion technology, especially in the field of code completion, where significant achievements have been made. This project focuses on pure text autocompletion, aiming to provide users with a text autocompletion feature. When users are editing text, the system will automatically complete the content after the cursor based on the current editing area and cursor position, helping users improve writing efficiency. We use a pretrained language model, combined with cursor position, context length, historical documents, and other factors, to generate personalized completions. Through this project, users will receive intelligent completions based on their writing style and the current text content, enhancing coherence and personalized experience in writing.

To achieve this task, we utilized a pretrained model to generate text completions. Based on this model, we built some special prompt techniques to guide the model in generating completions that align with the current context and historical documents. For evaluation, we used the publicly available news dataset [bbc_news_alltime](https://huggingface.co/datasets/RealTimeData/bbc_news_alltime) as the evaluation data. This dataset provides rich text resources, making it suitable for constructing evaluation data related to writing scenarios. In the experiments, we considered multiple factors to personalize the text completions, such as cursor position (whether the cursor is in the middle of a sentence or at the end), context length (the effect of different lengths of surrounding text on the completion), completion length (how long the completion should be), and the use of historical documents, to comprehensively evaluate the quality of the generated text.

## 2. Detailed Implementation Report

### 2.1 Model Selection and Prompt Construction

In this project, we chose the pretrained model [Mistral-7B-Instruct-v0.2](https://huggingface.co/mistralai/Mistral-7B-Instruct-v0.2) for text completion. When selecting the model, we compared several models, such as [distilgpt2](https://huggingface.co/distilbert/distilgpt2), and ultimately decided to use Mistral-7B-Instruct-v0.2. The main reason for this choice is that it can handle relatively long input texts (about 32k tokens), which is very suitable for scenarios that require input of large historical texts. Additionally, Mistral-7B's API is stable, and its performance is reliable, making it suitable for practical applications.

For prompt construction, we have not found an existing model that can precisely specify the context input, so we adopted a simple stacking method, where we concatenate historical documents, the following text, and the previous text as input. We place the current text at the end, hoping that the generated content will directly connect to the end of the current text, making the completion more coherent and natural.

This method is simple and effective, and any model supporting text generation can leverage it. However, one clear downside is that the following text becomes part of the previous text. A better solution would be to fine-tune the model and design special prompt structures to make the model more accurately understand and generate context-related content. However, due to time constraints and resource limitations, we opted to directly call the pretrained model and use a simplified prompt construction approach. This allowed us to focus on implementing the core functionality of text completion without delving deeply into model training and optimization.

Additionally, in practical applications, due to the uncertainty in the number of historical documents, we cannot guarantee that the total input length will always be less than the model's maximum input limit. Our approach is to truncate when the historical documents, following text, and previous text exceed this limit (currently set to 8000 tokens). Specifically, we keep the earlier portions of the historical documents and following text, while for the previous text, we retain the later portions. This approach ensures that we keep the content closest to the current cursor position, improving the relevance and naturalness of the completion.

Finally, we set different parameters for the model to generate completions of varying lengths. Specifically, we set different values for the `max_new_tokens`, `min_new_tokens`, `stop`, and `length_penalty` parameters when `completion_length` is set to `short`, `medium`, or `long`.

The model invocation code can be found in the `TextCompleter` class's `_call_generator_api` method in `src/text_completer.py`. The prompt construction can be found in the `TextCompleter` class's `_build_prompt` and `complete_text` methods in `src/text_completer.py`. For different completion lengths, refer to the `completion_length` parameter in the `complete_text` method. To experience the functionality, you can run the `toy_example.py` file.

### 2.2 Evaluation Data Processing

We process the evaluation data using the [bbc_news_alltime dataset](https://huggingface.co/datasets/RealTimeData/bbc_news_alltime) in batches. Each batch contains: 3 historical documents and 1 test document. The special feature of this dataset is that each news article consists of multiple paragraphs, and each paragraph is relatively short. Therefore, we handle it as follows:

For context length `context_length`, when `context_length` is "no", we randomly select 2 consecutive paragraphs; when `context_length` is "short", we randomly select 4 paragraphs; when `context_length` is "medium", we select 8 paragraphs; and when `context_length` is "long", we select the entire text. For cursor position `cursor_position`, if the selected following text contains $m$ paragraphs, we use the content of more than $(m // 2) + 1$ paragraphs as the following text. If the cursor position is "sentence_middle", we randomly select a position within the $(m // 2)$-th paragraph that is not at the sentence end, and use it along with the preceding text as the previous text. If the cursor position is "sentence_end", we randomly select a sentence-ending point in the $(m // 2)$-th paragraph and use it along with the preceding text as the previous text. It is worth noting that we allow the previous text not to end with a complete word. For example, if the previous text is "I enjoy pl", we may expect it to generate "aying games", completing the sentence as "I enjoy playing games" to test the word completion functionality.

The handling of context length is described in the `get_context_samples` function in `src/text_processors.py`. The handling of cursor position is described in the `split_by_cursor_position` function in `src/text_processors.py`. Historical documents are passed as the `history_docs` parameter to the `TextCompleter` class's `complete_text` method in `src/text_completer.py`. If it is `None` or an empty list, it indicates that no historical documents are used.

## 3. Areas for Improvement

Due to time, computational resources, and my current knowledge limitations, this project is currently a very simple practice. During the implementation process, we focused on quickly implementing a basic text completion system using a pretrained model for text generation, with simple context handling to meet different scenario requirements. However, this implementation still has many areas that can be further improved and optimized. In addition to selecting more advanced language models, here are some potential improvement directions for future exploration:

### 3.1 Handling Long Historical Documents

Since the number of historical documents can be quite large, blindly truncating them may cause some key information to be lost, which would affect the quality of the generated text. To address this, we can introduce some filtering mechanisms to select historical documents that are more relevant to the current text. Specifically, we can use text similarity measures to assess the relevance between the current text and the historical documents, prioritizing those that are most closely related to the current editing area for processing.

Another improvement method is to introduce a separate model specifically designed for summarization, such as [BART](https://huggingface.co/facebook/bart-large-cnn). Using this model, we can shorten long historical documents into summaries, retaining the most important parts, which would reduce the input text length while ensuring that key information from the historical documents can be effectively passed to the generation model. This would not only avoid issues related to long documents being truncated but also make the generation model more focused on the most relevant information, improving the personalization and coherence of the generated content. These processing methods can help the generation model consider the relevant information from historical documents more comprehensively, and avoid performance degradation due to information overload. By selecting more appropriate historical documents and summarizing them, we might further improve the quality of the model's output.

### 3.2 Constructing Prompts

Currently, we handle historical documents and context by simply stacking them, combining historical documents, previous text, and following text into a unified input for the model. While this approach can address basic context issues, it does not fully leverage the characteristics of each part, and may cause the model to ignore the different roles and importance of each part. For example, the historical documents provide background and reference for the current text, while the previous text directly affects the generated content, and the following text needs to be generated based on the cursor position.

To improve this, we can fine-tune the model and introduce specific prompts to clearly differentiate the roles of each part. During the fine-tuning process, we can design a task for the model to learn how to handle and differentiate historical documents, previous text, and following text. For example, we can add special keywords such as “[HISTORY]” for historical documents, and “[PREVIOUS]” and “[NEXT]” for the previous and following text, respectively. These special prompts will help the model recognize the different roles of each part, allowing the model to optimize the generation based on the part's function. In this way, the model can better understand which parts are historical information, which are direct context, and generate more logical and coherent text accordingly.

### 3.3 Text Generation Issues

Currently, we control the length of generated text using preset parameters, but this method sometimes leads to incomplete sentences, especially in the "long" case. To address this issue, we can introduce [stopping_criteria](https://huggingface.co/docs/transformers.js/v3.0.0/en/api/generation/stopping_criteria) to better control the stopping conditions for text generation.


### 3.4 Selection of Test Data

In the current implementation, we randomly selected historical documents for testing. However, this approach may result in poor relevance between the historical documents and the test document, which could affect the evaluation of the completion results. Therefore, to improve the effectiveness of the testing, we can select historical documents that are more relevant to the test document by calculating the similarity between the documents based on the content and context of the test document. This approach ensures that the historical documents used for generating the completion provide more relevant background information, leading to a better evaluation of the model's performance.

Additionally, we could consider introducing other types of datasets for comparison, such as academic papers, literary works, etc. These datasets typically contain more structured and in-depth texts, which can provide the model with different contexts and writing styles, offering more dimensions for evaluation. By comparing different types of datasets, we can further analyze the model's performance in various writing scenarios and optimize the text completion effect for different contexts.

### 3.5 Selection of Evaluation Metrics

Currently, we rely solely on manual evaluation to assess the model's generation performance, lacking systematic and objective evaluation metrics. In the future, we could introduce numerical metrics, such as Perplexity, to more intuitively assess the model's performance in personalized text generation. These metrics would help us better quantify the generation results, improving the comprehensiveness and objectivity of the evaluation.

